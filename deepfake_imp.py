# -*- coding: utf-8 -*-
"""deepfake_Imp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11R83DrM3cWVqxGdJoPcaV0R9lrBTGiE2
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'deepfakevidny:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5066806%2F8492414%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240524%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240524T120749Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5315fbb0e0901e37c43ddc55817a5b87221831aabec78f8a88b4a6dcfeb2713c048941ccecad79e25b83dc0ed6abe5086062103d51c4c256dc15edc315c96dc2a6832174e911fe4987c6c1f499456a6bd0ba27cc0bc502b4e6e540420a20236b5dc4c28332387b99a45136aaedde40bb1b1b5989140583ac2767a158cd3a092bfed62fc87625db2cf973ae50c4e8d05d77100c6c43f70f45bf4f79fae0cc35953d4179742bedb35899796b9444727d51358402b0bbfc1b3f44c3eb9a7c3147f533bc51f945b9e49651f7eb73e14f6b36807e761d9be2b20c36e2bfb6855f3b591b1470bef8828352711c7e006e85465fdb3d319fe2aae0eb0751ddfec7d07f8b'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import tensorflow as tf

print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'deepfakevidny:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5066806%2F8492414%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240523%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240523T231444Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D088237a1c6541375de7e2b4f7a67ec01dd93c2760a1a6bd11aefea81ca34583e7d3a6d753b3d1dca4a2b7013d4882baf6e257e32d60973f14ad141764c4923066fe9b9804c25d2890a94e3f74fda9519a759914647d5a8d1a66e8de2eb5ed3cd1feb0f625a62f2c367f0e423c0c3b4361d552bde4fc70f7414904ea0e18089b669238d71a5aaad6d38c864519f8e5a3a2d0c1e02f301eb21d0c8f9a6687260dcfee6a8313d4fc523ae34b48e2d8043817b4624b543c7c8fb09b5fb7ee8b3f2ad2d02ebd4bfde9fe9dffced3764ec697192ffa8fc9164bdb2818a442f5765a6cdf3ac03ad0df1f9fffbf12e1a196f51fd604d0ea2ceacee98e5c21f9a8c60108c'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

!pip install dlib==19.22.0

import os
import cv2
import dlib
import csv
import json
from tqdm import tqdm

# Path to the train videos
train_frame_folder = '/kaggle/input/deepfakevidny/deepfake-detection-challenge/train_sample_videos'

# Path to save extracted frames
real_save_path = '/kaggle/working/dataset/real/'
fake_save_path = '/kaggle/working/dataset/fake/'

# Create directories if they don't exist
os.makedirs(real_save_path, exist_ok=True)
os.makedirs(fake_save_path, exist_ok=True)

metadata_path = '/kaggle/input/deepfakevidny/deepfake-detection-challenge/train_sample_videos/metadata.json'
with open(metadata_path, 'r') as file:
    metadata = json.load(file)

# Display metadata to verify
print(metadata)

import os
import cv2
import dlib
from tqdm import tqdm

# Initialize face detector
detector = dlib.get_frontal_face_detector()

# Get the list of train videos
list_of_train_data = [f for f in os.listdir(train_frame_folder) if f.endswith('.mp4')]

# Process each video
for vid in tqdm(list_of_train_data):
    count = 0
    cap = cv2.VideoCapture(os.path.join(train_frame_folder, vid))
    frameRate = cap.get(cv2.CAP_PROP_FPS)  # Get the frame rate of the video

    if vid in metadata:
        save_path = real_save_path if metadata[vid] == 'REAL' else fake_save_path
    else:
        print(f"Video '{vid}' not found in metadata. Skipping...")
        continue

    while cap.isOpened():
        frameId = cap.get(cv2.CAP_PROP_POS_FRAMES)  # Current frame number
        ret, frame = cap.read()
        if not ret:
            break
        if frameId % ((int(frameRate)+1)*1) == 0:
            face_rects, scores, idx = detector.run(frame, 0)
            for i, d in enumerate(face_rects):
                x1 = d.left()
                y1 = d.top()
                x2 = d.right()
                y2 = d.bottom()
                crop_img = frame[y1:y2, x1:x2]
                save_filename = os.path.join(save_path, f"{vid.split('.')[0]}_{count}.png")
                cv2.imwrite(save_filename, cv2.resize(crop_img, (128, 128)))
                count += 1

    cap.release()

import dlib
import cv2
import os
import re
import json
from pylab import *
from PIL import Image, ImageChops, ImageEnhance

import csv

metadata_path = '/kaggle/input/deepfakevidny/metadata.csv (2)/metadata.csv'
metadata = {}
with open(metadata_path, 'r') as file:
    reader = csv.DictReader(file)
    for row in reader:
        metadata[row['videoname']] = row['label']

# Print a few entries to verify
print(list(metadata.items())[:5])

import os

real_save_path = '/kaggle/working/dataset/real/'
fake_save_path = '/kaggle/working/dataset/fake/'

# Create directories if they don't exist
os.makedirs(real_save_path, exist_ok=True)
os.makedirs(fake_save_path, exist_ok=True)

# Check if directories are created
print(os.path.exists(real_save_path))
print(os.path.exists(fake_save_path))

import os

train_frame_folder = '/kaggle/input/deepfakevidny/deepfake-detection-challenge/train_sample_videos'
list_of_train_data = [f for f in os.listdir(train_frame_folder) if f.endswith('.mp4')]

# Print a few video filenames to verify
print(list_of_train_data[:5])

import os
import cv2
import dlib
from tqdm import tqdm

# Paths
train_frame_folder = '/kaggle/input/deepfakevidny/deepfake-detection-challenge/train_sample_videos'
metadata_path = '/kaggle/input/deepfakevidny/deepfake-detection-challenge/train_sample_videos/metadata.json'
real_save_path = '/kaggle/working/dataset/real/'
fake_save_path = '/kaggle/working/dataset/fake/'

# Create directories if they don't exist
os.makedirs(real_save_path, exist_ok=True)
os.makedirs(fake_save_path, exist_ok=True)

# Load metadata
metadata = {}
with open(metadata_path, 'r') as file:
    reader = csv.DictReader(file)
    for row in reader:
        metadata[row['videoname']] = row['label']

# Get the list of train videos
list_of_train_data = [f for f in os.listdir(train_frame_folder) if f.endswith('.mp4')]

# Initialize face detector
detector = dlib.get_frontal_face_detector()

# Process only the first video for testing
if list_of_train_data:
    vid = list_of_train_data[0]
    count = 0
    cap = cv2.VideoCapture(os.path.join(train_frame_folder, vid))

    # Check if video can be opened
    if not cap.isOpened():
        print(f"Could not open video '{vid}'. Skipping...")
    else:
        # Get the frame rate of the video
        frameRate = cap.get(cv2.CAP_PROP_FPS)

        # Check if vid is in metadata
        if vid not in metadata:
            print(f"Video '{vid}' not found in metadata. Skipping...")
        else:
            # Determine save path based on label
            save_path = real_save_path if metadata[vid] == 'REAL' else fake_save_path

            while cap.isOpened():
                frameId = cap.get(cv2.CAP_PROP_POS_FRAMES)  # Current frame number
                ret, frame = cap.read()
                if not ret:
                    break
                if frameId % ((int(frameRate)+1)*1) == 0:
                    face_rects, scores, idx = detector.run(frame, 0)
                    for i, d in enumerate(face_rects):
                        x1 = d.left()
                        y1 = d.top()
                        x2 = d.right()
                        y2 = d.bottom()
                        crop_img = frame[y1:y2, x1:x2]
                        save_filename = os.path.join(save_path, f"{vid.split('.')[0]}_{count}.png")
                        cv2.imwrite(save_filename, cv2.resize(crop_img, (128, 128)))
                        count += 1

        cap.release()

print("Test processing complete.")

train_frame_folder = '/kaggle/input/deepfakevidny/deepfake-detection-challenge/train_sample_videos'
with open(os.path.join(train_frame_folder, 'metadata.json'), 'r') as file:
    data = json.load(file)
list_of_train_data = [f for f in os.listdir(train_frame_folder) if f.endswith('.mp4')]
detector = dlib.get_frontal_face_detector()
for vid in list_of_train_data:
    count = 0
    cap = cv2.VideoCapture(os.path.join(train_frame_folder, vid))
    frameRate = cap.get(5)
    while cap.isOpened():
        frameId = cap.get(1)
        ret, frame = cap.read()
        if ret != True:
            break
        if frameId % ((int(frameRate)+1)*1) == 0:
            face_rects, scores, idx = detector.run(frame, 0)
            for i, d in enumerate(face_rects):
                x1 = d.left()
                y1 = d.top()
                x2 = d.right()
                y2 = d.bottom()
                crop_img = frame[y1:y2, x1:x2]
                if data[vid]['label'] == 'REAL':
                    cv2.imwrite('/kaggle/working/dataset/real/'+vid.split('.')[0]+'_'+str(count)+'.png', cv2.resize(crop_img, (128, 128)))
                elif data[vid]['label'] == 'FAKE':
                    cv2.imwrite('/kaggle/working/dataset/fake/'+vid.split('.')[0]+'_'+str(count)+'.png', cv2.resize(crop_img, (128, 128)))
                count+=1

import os
import cv2
import json
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sn
import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

import os
import json
import cv2
import dlib

train_frame_folder = '/kaggle/input/deepfakevidny/deepfake-detection-challenge/train_sample_videos'
output_real = '/kaggle/working/dataset/real'
output_fake = '/kaggle/working/dataset/fake'

# Create directories if they don't exist
os.makedirs(output_real, exist_ok=True)
os.makedirs(output_fake, exist_ok=True)

# Load metadata
metadata_path = os.path.join(train_frame_folder, 'metadata.json')
if not os.path.exists(metadata_path):
    raise FileNotFoundError(f"Metadata file not found: {metadata_path}")

with open(metadata_path, 'r') as file:
    data = json.load(file)

# Get list of video files
list_of_train_data = [f for f in os.listdir(train_frame_folder) if f.endswith('.mp4')]
if not list_of_train_data:
    raise FileNotFoundError("No video files found in the specified directory.")

detector = dlib.get_frontal_face_detector()

def process_video(vid):
    count = 0
    cap = cv2.VideoCapture(os.path.join(train_frame_folder, vid))
    frameRate = cap.get(cv2.CAP_PROP_FPS)
    while cap.isOpened():
        frameId = cap.get(cv2.CAP_PROP_POS_FRAMES)
        ret, frame = cap.read()
        if not ret:
            break
        if frameId % int(frameRate) == 0:  # Process one frame per second
            face_rects, _, _ = detector.run(frame, 0)
            for i, d in enumerate(face_rects):
                x1, y1, x2, y2 = d.left(), d.top(), d.right(), d.bottom()
                crop_img = frame[y1:y2, x1:x2]
                crop_img = cv2.resize(crop_img, (128, 128))
                label = data[vid]['label']
                output_folder = output_real if label == 'REAL' else output_fake
                output_path = os.path.join(output_folder, f"{vid.split('.')[0]}_{count}.png")
                cv2.imwrite(output_path, crop_img)
                count += 1
    cap.release()

# Process each video file sequentially
for vid in list_of_train_data:
    print(f"Processing video: {vid}")
    process_video(vid)

print("Processing complete.")

import os
import numpy as np
from tensorflow.keras.preprocessing.image import img_to_array, load_img
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

# Input shape
input_shape = (128, 128, 3)

# Dataset directory
data_dir = '/kaggle/working/dataset'

# Get list of real and fake image files
real_data = [f for f in os.listdir(os.path.join(data_dir, 'real')) if f.endswith('.png')]
fake_data = [f for f in os.listdir(os.path.join(data_dir, 'fake')) if f.endswith('.png')]

# Initialize lists for images and labels
X = []
Y = []

# Load real images and assign label 1
for img in real_data:
    img_path = os.path.join(data_dir, 'real', img)
    img_array = img_to_array(load_img(img_path)) / 255.0  # Normalize here
    X.append(img_array)
    Y.append(1)

# Load fake images and assign label 0
for img in fake_data:
    img_path = os.path.join(data_dir, 'fake', img)
    img_array = img_to_array(load_img(img_path)) / 255.0  # Normalize here
    X.append(img_array)
    Y.append(0)

# Convert lists to arrays
X = np.array(X)
Y = to_categorical(Y, 2)

# Train-test split
X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=5)

# Print shapes to verify
print(f"X_train shape: {X_train.shape}")
print(f"Y_train shape: {Y_train.shape}")
print(f"X_val shape: {X_val.shape}")
print(f"Y_val shape: {Y_val.shape}")

print(X)

print(Y_train)

"""# InceptionResNetV2"""

import tensorflow as tf
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras import optimizers
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.applications import InceptionResNetV2

# Enable eager execution for detailed error messages
tf.config.run_functions_eagerly(True)

input_shape = (128, 128, 3)
googleNet_model = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=input_shape)
googleNet_model.trainable = True

model = Sequential()
model.add(googleNet_model)
model.add(GlobalAveragePooling2D())
model.add(Dense(units=2, activation='softmax'))

optimizer = optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False)

# Compile the model with the reinitialized optimizer
model.compile(loss='binary_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

# Print model summary
model.summary()

# Ensure data types and shapes
print("X_train shape:", X_train.shape, "dtype:", X_train.dtype)
print("Y_train shape:", Y_train.shape, "dtype:", Y_train.dtype)
print("X_val shape:", X_val.shape, "dtype:", X_val.dtype)
print("Y_val shape:", Y_val.shape, "dtype:", Y_val.dtype)

# Check for None values in labels
print("Any None values in Y_train:", np.any(Y_train == None))
print("Any None values in Y_val:", np.any(Y_val == None))

# Add early stopping and learning rate reduction
early_stopping = EarlyStopping(monitor='val_loss',
                               min_delta=0,
                               patience=5,
                               verbose=1, mode='auto')

reduce_lr = ReduceLROnPlateau(monitor='val_loss',
                              factor=0.2,
                              patience=2,
                              verbose=1,
                              min_lr=1e-7)

# Train the model with verbose logging
EPOCHS = 20
BATCH_SIZE = 100
history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,
                    validation_data=(X_val, Y_val), verbose=1,
                    callbacks=[early_stopping, reduce_lr])

import tensorflow as tf
from google.colab import files
import json

# Assign the model to another variable
inception_resnet_v2_model = model

# Save the model in Keras native format
model_name = 'inception_resnet_v2_model.keras'
model.save(model_name)

# Convert history to JSON serializable format
history_serializable = {key: [float(value) for value in values] for key, values in history.history.items()}

# Save the training history
history_file = 'inception_resnet_v2_model_training_history.json'
with open(history_file, 'w') as f:
    json.dump(history_serializable, f)

# Download the model file
files.download(model_name)

# Download the history file
files.download(history_file)

import matplotlib.pyplot as plt

# Assuming 'history' and 'EPOCHS' are defined somewhere in your code

f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4))
t = f.suptitle('Pre-trained InceptionResNetV2 Transfer Learn with Fine-Tuning & Image Augmentation Performance', fontsize=12)
f.subplots_adjust(top=0.85, wspace=0.3)

epoch_list = list(range(1, EPOCHS + 1))
ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')
ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')
ax1.set_xticks(np.arange(0, EPOCHS + 1, 1))
ax1.set_ylabel('Accuracy Value')
ax1.set_xlabel('Epoch #')
ax1.set_title('Accuracy')
l1 = ax1.legend(loc="best")

ax2.plot(epoch_list, history.history['loss'], label='Train Loss')
ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')
ax2.set_xticks(np.arange(0, EPOCHS + 1, 1))
ax2.set_ylabel('Loss Value')
ax2.set_xlabel('Epoch #')
ax2.set_title('Loss')
l2 = ax2.legend(loc="best")

plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

Y_org = np.argmax(Y, axis=1) # Convert one-hot encoded labels to single labels

# Splitting data again to maintain the original labels for validation set
X_train, X_val, Y_train, Y_val, Y_train_org, Y_val_org = train_test_split(X, Y, Y_org, test_size=0.2, random_state=5)

# Function to plot confusion matrix
def print_confusion_matrix(y_true, y_pred, class_names):
    cm = confusion_matrix(y_true, y_pred)
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.ylabel('Actual', size=16)
    plt.xlabel('Predicted', size=16)
    plt.yticks(np.arange(len(class_names)) + 0.5, class_names, size=16)
    plt.xticks(np.arange(len(class_names)) + 0.5, class_names, size=16)
    plt.ylim(len(class_names), 0)
    plt.show()

# Making predictions on validation data
Y_val_pred = np.argmax(model.predict(X_val), axis=1)

# Plot the confusion matrix
print_confusion_matrix(Y_val_org, Y_val_pred, ['Fake', 'Real'])

import numpy as np
from sklearn.metrics import confusion_matrix

def print_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    print('Confusion Matrix:')
    print(cm)
    print('True positive = ', cm[0][0])
    print('False positive = ', cm[0][1])
    print('False negative = ', cm[1][0])
    print('True negative = ', cm[1][1])

# Assuming Y_val_org is your original labels (not one-hot encoded)
# Get the predicted probabilities
Y_pred_prob = model.predict(X_val)

# Convert probabilities to class predictions
Y_pred = np.argmax(Y_pred_prob, axis=1)

# Assuming Y_val_org is the original labels (not one-hot encoded)
Y_val_org = np.argmax(Y_val, axis=1)

# Print confusion matrix
print_confusion_matrix(Y_val_org, Y_pred)

TP = 753
FP = 67
FN = 123
TN = 33

accuracy = (TP + TN) / (TP + TN + FP + FN)
print(f'Accuracy: {accuracy:.3f}')

precision = TP / (TP + FP)
print(f'Precision: {precision:.3f}')

recall = TP / (TP + FN)
print(f'Recall: {recall:.3f}')

specificity = TN / (TN + FP)
print(f'Specificity: {specificity:.3f}')

import cv2
import dlib
import numpy as np
from tensorflow.keras.preprocessing.image import img_to_array
from google.colab.patches import cv2_imshow

def predict_frame(model, frame):
    detector = dlib.get_frontal_face_detector()
    face_rects, scores, idx = detector.run(frame, 0)
    predictions = []

    for i, d in enumerate(face_rects):
        x1 = d.left()
        y1 = d.top()
        x2 = d.right()
        y2 = d.bottom()
        crop_img = frame[y1:y2, x1:x2]
        data = img_to_array(cv2.resize(crop_img, (128, 128))).flatten() / 255.0
        data = data.reshape(-1, 128, 128, 3)

        # Get predicted probabilities
        predictions_prob = model.predict(data)
        # Convert probabilities to class predictions
        prediction = np.argmax(predictions_prob, axis=1)
        predictions.append(prediction[0])

    return predictions

def test_video(model, video_path):
    cap = cv2.VideoCapture(video_path)
    frameRate = cap.get(5)  # frame rate
    while cap.isOpened():
        frameId = cap.get(1)  # current frame number
        ret, frame = cap.read()
        if not ret:
            break
        if frameId % ((int(frameRate) + 1) * 1) == 0:
            predictions = predict_frame(model, frame)
            for prediction in predictions:
                print(f"Prediction for frame {frameId}: {prediction}")

            # Display the frame
            cv2_imshow(frame)

    cap.release()

# Example usage
video_path = '../kaggle/input/deepfakevidny/deepfake-detection-challenge/test_videos/jawgcggquk.mp4'
test_video(model, video_path)

"""# EfficientNetB4"""

import tensorflow as tf
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras import optimizers
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.applications import EfficientNetB4
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
from google.colab import files
import matplotlib.pyplot as plt
import numpy as np

# Enable eager execution for detailed error messages
tf.config.run_functions_eagerly(True)

# Define your model
input_shape = (128, 128, 3)
efficient_net_model = EfficientNetB4(include_top=False, weights='imagenet', input_shape=input_shape)
efficient_net_model.trainable = True

model = Sequential()
model.add(efficient_net_model)
model.add(GlobalAveragePooling2D())
model.add(Dropout(0.5))  # Add dropout layer for regularization
model.add(Dense(units=2, activation='softmax'))

# Reinitialize the optimizer
optimizer = optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False)

# Compile the model with the reinitialized optimizer
model.compile(loss='binary_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

# Print model summary
model.summary()

# Ensure data types and shapes
print("X_train shape:", X_train.shape, "dtype:", X_train.dtype)
print("Y_train shape:", Y_train.shape, "dtype:", Y_train.dtype)
print("X_val shape:", X_val.shape, "dtype:", X_val.dtype)
print("Y_val shape:", Y_val.shape, "dtype:", Y_val.dtype)

# Check for None values in labels
print("Any None values in Y_train:", np.any(Y_train == None))
print("Any None values in Y_val:", np.any(Y_val == None))

# Add early stopping and learning rate reduction
early_stopping = EarlyStopping(monitor='val_loss',
                               min_delta=0,
                               patience=5,
                               verbose=1, mode='auto')

reduce_lr = ReduceLROnPlateau(monitor='val_loss',
                              factor=0.5,
                              patience=3,
                              verbose=1,
                              min_lr=1e-7)

# Data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest')

datagen.fit(X_train)

# Train the model with verbose logging
EPOCHS = 20
BATCH_SIZE = 100
history = model.fit(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),
                    steps_per_epoch=len(X_train) // BATCH_SIZE,
                    epochs=EPOCHS,
                    validation_data=(X_val, Y_val),
                    verbose=1,
                    callbacks=[early_stopping, reduce_lr])

# Assign the model to another variable
efficient_net_model_b4 = model

# Save the model in Keras native format
model_name = 'efficient_net_model_b4.keras'
model.save(model_name)

# Convert history to JSON serializable format
history_serializable = {key: [float(value) for value in values] for key, values in history.history.items()}

# Save the training history
history_file = 'efficient_net_model_b4_training_history.json'
with open(history_file, 'w') as f:
    json.dump(history_serializable, f)

# Download the model file
#files.download(model_name)

# Download the history file
files.download(history_file)

# Plot training history
f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4))
t = f.suptitle('Pre-trained EfficientNetB4 Transfer Learn with Fine-Tuning & Image Augmentation Performance', fontsize=12)
f.subplots_adjust(top=0.85, wspace=0.3)

# Adjust the length of the epoch_list to the actual number of epochs run
actual_epochs = len(history.history['accuracy'])
epoch_list = list(range(1, actual_epochs + 1))
ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')
ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')
ax1.set_xticks(np.arange(0, actual_epochs + 1, 1))
ax1.set_ylabel('Accuracy Value')
ax1.set_xlabel('Epoch #')
ax1.set_title('Accuracy')
l1 = ax1.legend(loc="best")

ax2.plot(epoch_list, history.history['loss'], label='Train Loss')
ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')
ax2.set_xticks(np.arange(0, actual_epochs + 1, 1))
ax2.set_ylabel('Loss Value')
ax2.set_xlabel('Epoch #')
ax2.set_title('Loss')
l2 = ax2.legend(loc="best")

plt.show()

# Function to plot confusion matrix
def print_confusion_matrix(y_true, y_pred, class_names):
    cm = confusion_matrix(y_true, y_pred)
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.ylabel('Actual', size=16)
    plt.xlabel('Predicted', size=16)
    plt.yticks(np.arange(len(class_names)) + 0.5, class_names, size=16)
    plt.xticks(np.arange(len(class_names)) + 0.5, class_names, size=16)
    plt.ylim(len(class_names), 0)
    plt.show()

# Assuming Y_val_org is your original labels (not one-hot encoded)
# Get the predicted probabilities
Y_pred_prob = model.predict(X_val)

# Convert probabilities to class predictions
Y_pred = np.argmax(Y_pred_prob, axis=1)

# Assuming Y_val_org is the original labels (not one-hot encoded)
Y_val_org = np.argmax(Y_val, axis=1)

# Plot the confusion matrix
print_confusion_matrix(Y_val_org, Y_pred, ['Class 0', 'Class 1'])

files.download(model_name)

"""# VGG16"""

import tensorflow as tf
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from tensorflow.keras import optimizers
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
from google.colab import files
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Enable mixed precision training
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# Define your model
input_shape = (128, 128, 3)
base_model = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)
model = Sequential()
model.add(base_model)
model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dense(512, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(2, activation='softmax', dtype='float32'))  # Set dtype to float32 for final layer

# Print model summary
model.summary()

# Reinitialize the optimizer
optimizer = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=False)

# Compile the model with the reinitialized optimizer
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Ensure data types and shapes
print("X_train shape:", X_train.shape, "dtype:", X_train.dtype)
print("Y_train shape:", Y_train.shape, "dtype:", Y_train.dtype)
print("X_val shape:", X_val.shape, "dtype:", X_val.dtype)
print("Y_val shape:", Y_val.shape, "dtype:", Y_val.dtype)

# Check for None values in labels
print("Any None values in Y_train:", np.any(Y_train == None))
print("Any None values in Y_val:", np.any(Y_val == None))

# Add early stopping, learning rate reduction, and model checkpointing
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-7)

model_checkpoint = ModelCheckpoint('vgg16_model_checkpoint.keras', monitor='val_loss', save_best_only=True, verbose=1)

# Data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest')

datagen.fit(X_train)

# Train the model with verbose logging
EPOCHS = 30
BATCH_SIZE = 128  # Try using a larger batch size with mixed precision
history = model.fit(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),
                    steps_per_epoch=len(X_train) // BATCH_SIZE,
                    epochs=EPOCHS,
                    validation_data=(X_val, Y_val),
                    verbose=1,
                    callbacks=[early_stopping, reduce_lr, model_checkpoint])

# Save the model in Keras native format
model_name = 'vgg16_model.keras'
model.save(model_name)

# Convert history to JSON serializable format
history_serializable = {key: [float(value) for value in values] for key, values in history.history.items()}

# Save the training history
history_file = 'vgg16_model_training_history.json'
with open(history_file, 'w') as f:
    json.dump(history_serializable, f)

# Download the model file
files.download(model_name)

# Download the history file
files.download(history_file)

# Plot training history
f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4))
t = f.suptitle('Pre-trained VGG16 Transfer Learn with Fine-Tuning & Image Augmentation Performance', fontsize=12)
f.subplots_adjust(top=0.85, wspace=0.3)

# Adjust the length of the epoch_list to the actual number of epochs run
actual_epochs = len(history.history['accuracy'])
epoch_list = list(range(1, actual_epochs + 1))
ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')
ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')
ax1.set_xticks(np.arange(0, actual_epochs + 1, 1))
ax1.set_ylabel('Accuracy Value')
ax1.set_xlabel('Epoch #')
ax1.set_title('Accuracy')
l1 = ax1.legend(loc="best")

ax2.plot(epoch_list, history.history['loss'], label='Train Loss')
ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')
ax2.set_xticks(np.arange(0, actual_epochs + 1, 1))
ax2.set_ylabel('Loss Value')
ax2.set_xlabel('Epoch #')
ax2.set_title('Loss')
l2 = ax2.legend(loc="best")

plt.show()

# Function to plot confusion matrix
def print_confusion_matrix(y_true, y_pred, class_names):
    cm = confusion_matrix(y_true, y_pred)
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.ylabel('Actual', size=16)
    plt.xlabel('Predicted', size=16)
    plt.yticks(np.arange(len(class_names)) + 0.5, class_names, size=16)
    plt.xticks(np.arange(len(class_names)) + 0.5, class_names, size=16)
    plt.ylim(len(class_names), 0)
    plt.show()

# Assuming Y_val_org is your original labels (not one-hot encoded)
# Get the predicted probabilities
Y_pred_prob = model.predict(X_val)

# Convert probabilities to class predictions
Y_pred = np.argmax(Y_pred_prob, axis=1)

# Assuming Y_val_org is the original labels (not one-hot encoded)
Y_val_org = np.argmax(Y_val, axis=1)

# Plot the confusion matrix
print_confusion_matrix(Y_val_org, Y_pred, ['Class 0', 'Class 1'])

files.download(model_name)

vgg16_model = model

from tensorflow.keras import backend as K
import tensorflow as tf
import gc

# Clear the Keras session
K.clear_session()

# Reset the default graph (useful for TensorFlow 1.x)
tf.compat.v1.reset_default_graph()

# Force garbage collection
gc.collect()

"""# Detailed Models Performance Analysis"""

import json
import matplotlib.pyplot as plt
import numpy as np

# Load training history from JSON files
def load_history(filename):
    with open(filename, 'r') as f:
        history = json.load(f)
    return history

# Paths to the JSON history files
inception_history_file = 'inception_resnet_v2_model_training_history.json'
efficientnet_history_file = 'efficient_net_model_b4_training_history.json'
vgg16_history_file = 'vgg16_model_training_history.json'

# Load histories
inception_history = load_history(inception_history_file)
efficientnet_history = load_history(efficientnet_history_file)
vgg16_history = load_history(vgg16_history_file)

# Plot training and validation accuracy
plt.figure(figsize=(20, 10))
plt.subplot(2, 2, 1)
plt.plot(inception_history['accuracy'], label='InceptionResNetV2 Train Accuracy')
plt.plot(inception_history['val_accuracy'], label='InceptionResNetV2 Val Accuracy')
plt.plot(efficientnet_history['accuracy'], label='EfficientNetB4 Train Accuracy')
plt.plot(efficientnet_history['val_accuracy'], label='EfficientNetB4 Val Accuracy')
plt.plot(vgg16_history['accuracy'], label='VGG16 Train Accuracy')
plt.plot(vgg16_history['val_accuracy'], label='VGG16 Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot training and validation loss
plt.subplot(2, 2, 2)
plt.plot(inception_history['loss'], label='InceptionResNetV2 Train Loss')
plt.plot(inception_history['val_loss'], label='InceptionResNetV2 Val Loss')
plt.plot(efficientnet_history['loss'], label='EfficientNetB4 Train Loss')
plt.plot(efficientnet_history['val_loss'], label='EfficientNetB4 Val Loss')
plt.plot(vgg16_history['loss'], label='VGG16 Train Loss')
plt.plot(vgg16_history['val_loss'], label='VGG16 Val Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Individual Model Analysis
models = {
    'InceptionResNetV2': inception_history,
    'EfficientNetB4': efficientnet_history,
    'VGG16': vgg16_history
}

# Adjust the subplot indices to fit within a 2x2 grid
subplot_indices = [3, 4]
for i, (model_name, history) in zip(subplot_indices, list(models.items())[:2]):
    plt.subplot(2, 2, i)
    plt.plot(history['accuracy'], label=f'{model_name} Train Accuracy')
    plt.plot(history['val_accuracy'], label=f'{model_name} Val Accuracy')
    plt.plot(history['loss'], label=f'{model_name} Train Loss')
    plt.plot(history['val_loss'], label=f'{model_name} Val Loss')
    plt.title(f'{model_name} Performance')
    plt.xlabel('Epochs')
    plt.ylabel('Metrics')
    plt.legend()

plt.tight_layout()
plt.show()

# Detailed Analysis and Justification
def analyze_model(history, model_name):
    final_train_acc = history['accuracy'][-1]
    final_val_acc = history['val_accuracy'][-1]
    final_train_loss = history['loss'][-1]
    final_val_loss = history['val_loss'][-1]

    print(f"\n{model_name} Analysis:")
    print(f"Final Training Accuracy: {final_train_acc:.4f}")
    print(f"Final Validation Accuracy: {final_val_acc:.4f}")
    print(f"Final Training Loss: {final_train_loss:.4f}")
    print(f"Final Validation Loss: {final_val_loss:.4f}")
    return final_val_acc, final_val_loss

# Compare models
inception_val_acc, inception_val_loss = analyze_model(inception_history, 'InceptionResNetV2')
efficientnet_val_acc, efficientnet_val_loss = analyze_model(efficientnet_history, 'EfficientNetB4')
vgg16_val_acc, vgg16_val_loss = analyze_model(vgg16_history, 'VGG16')

# Determine the best model
best_model_name = 'InceptionResNetV2' if inception_val_acc > efficientnet_val_acc and inception_val_acc > vgg16_val_acc else 'EfficientNetB4' if efficientnet_val_acc > vgg16_val_acc else 'VGG16'
print(f"\nThe best model based on validation accuracy is: {best_model_name}")

# Justification
if best_model_name == 'InceptionResNetV2':
    justification = f"InceptionResNetV2 achieved the highest validation accuracy of {inception_val_acc:.4f}, with a final validation loss of {inception_val_loss:.4f}. This indicates that the model generalizes well to the validation data and has learned the patterns effectively."
elif best_model_name == 'EfficientNetB4':
    justification = f"EfficientNetB4 achieved the highest validation accuracy of {efficientnet_val_acc:.4f}, with a final validation loss of {efficientnet_val_loss:.4f}. This model balances performance and computational efficiency."
else:
    justification = f"VGG16 achieved the highest validation accuracy of {vgg16_val_acc:.4f}, with a final validation loss of {vgg16_val_loss:.4f}. Despite being an older architecture, VGG16 has proven to be highly effective for this task."

print("\nJustification:")
print(justification)

"""# Deployment: Test on Sample Video"""

# Example usage
video_path = '../kaggle/input/deepfakevidny/deepfake-detection-challenge/test_videos/jawgcggquk.mp4'
test_video(vgg16_model, video_path)

# Example usage
video_path = '../kaggle/input/deepfakevidny/deepfake-detection-challenge/test_videos/aktnlyqpah.mp4'
test_video(efficient_net_model_b4, video_path)